{"cells":[{"cell_type":"markdown","id":"31a98c07","metadata":{"id":"31a98c07"},"source":["Download the BookCorpus dataset. Take every 7-th sample (the indices are multiple of 7:[0,7,14,21,...]) from the entire dataset. This will result in a dataset with 10 million samples (exactly, 10,572,033). Use these samples to build a tokenizer with the BPE tokenization algorithm by varying the vocabulary size.\n","\n","Normalizer: LowerCase\n","\n","PreTokenizer: WhiteSpace\n","\n","Model: BPE\n","\n","Special tokens: [GO],[UNK],[PAD],[EOS]\n","\n","PostProcessing: None\n","\n","Tokenize the input text: “SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.” using the following configurations.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"cbe1034c","metadata":{"id":"cbe1034c"},"outputs":[],"source":["ga_text=\"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\""]},{"cell_type":"code","execution_count":null,"id":"c2fc1eb4","metadata":{"id":"c2fc1eb4"},"outputs":[],"source":["## Dataset\n","from pprint import pprint\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.normalizers import Lowercase\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import  BpeTrainer\n","from transformers import PreTrainedTokenizerFast\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"id":"90d85745","metadata":{"scrolled":true,"id":"90d85745","outputId":"8b68dead-e17b-4516-f069-3df5248b03cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["# downloading the bookcorpus dataset\n","\n","ds = load_dataset(\"bookcorpus\", split=\"all\")"]},{"cell_type":"code","execution_count":null,"id":"c7bb2499","metadata":{"id":"c7bb2499"},"outputs":[],"source":["# select every 7th sample, (exactly, 10,572,033)\n","ids = range(0, len(ds), 7)\n","ds_new = ds.select(ids)"]},{"cell_type":"code","execution_count":null,"id":"8ac559d8","metadata":{"id":"8ac559d8","outputId":"3bd64a05-2243-4455-d9f5-baa2609ee920"},"outputs":[{"data":{"text/plain":["10572033"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(ds_new)"]},{"cell_type":"code","execution_count":null,"id":"95ea5ce5","metadata":{"id":"95ea5ce5"},"outputs":[],"source":["# build the BPE tokenizer\n","model = BPE(unk_token=\"[UNK]\")\n","tokenizer = Tokenizer(model)\n","tokenizer.normalizer = Lowercase()\n","tokenizer.pre_tokenizer = Whitespace()"]},{"cell_type":"code","execution_count":null,"id":"c6d9d99a","metadata":{"id":"c6d9d99a"},"outputs":[],"source":["def get_batch(batch_size=1000):\n","    for i in range(0, len(ds_new), batch_size):\n","        yield ds_new[i: i+batch_size][\"text\"]"]},{"cell_type":"code","execution_count":null,"id":"495ebf7c","metadata":{"id":"495ebf7c","outputId":"84adaadb-3d73-49f3-99d3-76807d82fb1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["8\n"]}],"source":["from multiprocessing import cpu_count\n","print(cpu_count())"]},{"cell_type":"markdown","id":"1e35efe4","metadata":{"id":"1e35efe4"},"source":["# Q1\n","\n","Keep the vocabulary size at 5000 and tokenize the input text using the learned vocabulary. Choose the number of tokens returned by the tokenizer."]},{"cell_type":"code","execution_count":null,"id":"18e781a7","metadata":{"id":"18e781a7"},"outputs":[],"source":["trainer = BpeTrainer(vocab_size=5000,\n","                     special_tokens=[\"[UNK]\",\"[GO]\",\"[PAD]\",\"[EOS]\"],\n","                     continuing_subword_prefix=\"##\"\n","                    )"]},{"cell_type":"code","execution_count":null,"id":"09d4dfaf","metadata":{"id":"09d4dfaf","outputId":"78b9a32c-ef61-44fb-d5e6-488f70f23513"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["tokenizer1 = deepcopy(tokenizer)\n","tokenizer1.train_from_iterator(get_batch(batch_size=10000),\n","                              trainer=trainer,\n","                              length=len(ds_new)\n","                            )"]},{"cell_type":"code","execution_count":null,"id":"a862b252","metadata":{"id":"a862b252","outputId":"afa7e6c6-e3c4-455c-bf73-800b7d434b97"},"outputs":[{"data":{"text/plain":["32"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["encoded = tokenizer1.encode(ga_text).tokens\n","len(encoded)"]},{"cell_type":"markdown","id":"390aa628","metadata":{"id":"390aa628"},"source":["# Q2\n","\n","Increase the vocabulary size to 10K, 15K and 32K. For each case, tokenize the same input with the newly learned vocabulary. Choose all the correct statements"]},{"cell_type":"code","execution_count":null,"id":"035bb302","metadata":{"scrolled":true,"id":"035bb302","outputId":"49520a75-33a8-49e4-e1c8-5d9c93bed4e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"data":{"text/plain":["28"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# vocab_size 10K\n","\n","tokenizer2 = deepcopy(tokenizer)\n","\n","trainer = BpeTrainer(vocab_size=10000,\n","                     special_tokens=[\"[UNK]\",\"[GO]\",\"[PAD]\",\"[EOS]\"],\n","                     continuing_subword_prefix=\"##\"\n","                    )\n","\n","tokenizer2.train_from_iterator(get_batch(batch_size=10000),\n","                              trainer=trainer,\n","                              length=len(ds_new))\n","\n","encoded = tokenizer2.encode(ga_text).tokens\n","len(encoded)"]},{"cell_type":"code","execution_count":null,"id":"8433abe9","metadata":{"id":"8433abe9","outputId":"50707d26-7fb7-4e74-cddb-6b4900b425d9"},"outputs":[{"data":{"text/plain":["10000"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer2.get_vocab_size()"]},{"cell_type":"code","execution_count":null,"id":"eb2ab4be","metadata":{"id":"eb2ab4be","outputId":"75c43fe1-f083-42a2-9c64-e62f6a51fd0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"data":{"text/plain":["28"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# vocab_size 15K\n","\n","tokenizer3 = deepcopy(tokenizer)\n","\n","\n","trainer = BpeTrainer(vocab_size=15000,\n","                     special_tokens=[\"[UNK]\",\"[GO]\",\"[PAD]\",\"[EOS]\"],\n","                     continuing_subword_prefix=\"##\"\n","                    )\n","\n","tokenizer3.train_from_iterator(get_batch(batch_size=10000),\n","                              trainer=trainer,\n","                              length=len(ds_new))\n","\n","encoded = tokenizer3.encode(ga_text).tokens\n","len(encoded)"]},{"cell_type":"code","execution_count":null,"id":"696443bd","metadata":{"id":"696443bd","outputId":"232bbf32-9dc2-4122-dbb2-5d5e078ec277"},"outputs":[{"data":{"text/plain":["15000"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer3.get_vocab_size()"]},{"cell_type":"code","execution_count":null,"id":"39da700a","metadata":{"id":"39da700a","outputId":"ff8d583b-60a7-4792-9747-9140ffbe3ede"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"data":{"text/plain":["25"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# vocab_size 32K\n","\n","tokenizer4 = deepcopy(tokenizer)\n","\n","\n","trainer = BpeTrainer(vocab_size=32000,\n","                     special_tokens=[\"[UNK]\",\"[GO]\",\"[PAD]\",\"[EOS]\"],\n","                     continuing_subword_prefix=\"##\"\n","                    )\n","\n","tokenizer4.train_from_iterator(get_batch(batch_size=10000),\n","                              trainer=trainer,\n","                              length=len(ds_new))\n","\n","encoded = tokenizer4.encode(ga_text).tokens\n","len(encoded)"]},{"cell_type":"code","execution_count":null,"id":"eb7b8d03","metadata":{"id":"eb7b8d03","outputId":"e3c2de52-407a-44fe-fe90-13c4c2ef50d7"},"outputs":[{"data":{"text/plain":["32000"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer4.get_vocab_size()"]},{"cell_type":"markdown","id":"33087363","metadata":{"id":"33087363"},"source":["# Q3\n","\n","\n","Download the pre-trained tokenizer file “hopper.json” used in the lecture, from [here](https://drive.google.com/file/d/1QNnyh8iMN-IqW_h1w8gAMtw09Em7-e1e/view) . The tokenizer was trained on all 70 million samples in the BookCorpus dataset. Tokenize the same input text using this “hopper” tokenizer. How many tokens are there?\n","\n","[After finding the answer, take a moment to compare the hopper tokenizer with the previous one]"]},{"cell_type":"code","execution_count":null,"id":"5a4ad8ba","metadata":{"id":"5a4ad8ba"},"outputs":[],"source":["pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"hopper.json\",\n","                                       unk_token=\"[UNK]\",\n","                                       pad_token=\"[PAD]\",\n","                                       model_input_names=[\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n","                                    )"]},{"cell_type":"code","execution_count":null,"id":"6394e5e5","metadata":{"id":"6394e5e5","outputId":"e6e28748-929d-44f6-b8b3-4d971974c06d"},"outputs":[{"name":"stdout","output_type":"stream","text":["25\n"]}],"source":["tokens = pt_tokenizer.encode(ga_text)\n","print(len(tokens))"]},{"cell_type":"code","source":[],"metadata":{"id":"QKSQdTcIrqVv"},"id":"QKSQdTcIrqVv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JtcGhtzKrp9q"},"id":"JtcGhtzKrp9q","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b4998374","metadata":{"id":"b4998374"},"source":["# Q4\n","\n","Suppose we know that the acronym “FY” will likely appear very frequently in most of the input text (assume the text comes from the financial domain). Therefore, we hope that adding it manually to the vocabulary might help. Add the token “FY” to the vocabulary and tokenize the input text. Enter the number of tokens produced.\n","\n","[Question to ponder: Does reducing the number of tokens helpful?]"]},{"cell_type":"code","execution_count":null,"id":"aac455ae","metadata":{"id":"aac455ae","outputId":"f4d5952d-1d80-4b00-90c4-c5d3c5ab18a7"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["pt_tokenizer.add_tokens(new_tokens=[\"FY\"])"]},{"cell_type":"code","execution_count":null,"id":"e418ed83","metadata":{"id":"e418ed83","outputId":"665c9969-084c-420b-d4c9-5a068dd776fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["22\n"]}],"source":["tokens = pt_tokenizer.encode(ga_text)\n","print(len(tokens))"]},{"cell_type":"markdown","id":"6e482a7b","metadata":{"id":"6e482a7b"},"source":["# Q5\n","\n","Load the “bert-base-uncased” and \"gpt2” tokenizers (use AutoTokenizer function from transformers). Which of the following special tokens are used in these tokenizers?\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5ff52c46","metadata":{"id":"5ff52c46"},"outputs":[],"source":["from transformers import AutoTokenizer"]},{"cell_type":"code","execution_count":null,"id":"6644a497","metadata":{"scrolled":false,"id":"6644a497"},"outputs":[],"source":["bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"id":"592fc2f5","metadata":{"id":"592fc2f5"},"outputs":[],"source":["gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"id":"3fe8820b","metadata":{"id":"3fe8820b","outputId":"8e3f3c41-bfdb-4b84-a7ca-3296be628c88"},"outputs":[{"data":{"text/plain":["{'bos_token': '<|endoftext|>',\n"," 'eos_token': '<|endoftext|>',\n"," 'unk_token': '<|endoftext|>'}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["gpt2_tokenizer.special_tokens_map"]},{"cell_type":"code","execution_count":null,"id":"46a5da01","metadata":{"id":"46a5da01","outputId":"3a900b37-c046-4da3-e541-b1d9bd3bd933"},"outputs":[{"data":{"text/plain":["{'unk_token': '[UNK]',\n"," 'sep_token': '[SEP]',\n"," 'pad_token': '[PAD]',\n"," 'cls_token': '[CLS]',\n"," 'mask_token': '[MASK]'}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["bert_tokenizer.special_tokens_map"]},{"cell_type":"markdown","id":"26ba0434","metadata":{"id":"26ba0434"},"source":["# Q6\n","\n","By now, we have four tokenizers.\n","\n","1. Custom tokenizer (vocab size 32K, trained on 10 million samples)\n","2. bert-base-uncased\n","3. gpt2\n","4. hopper\n","\n","Use these four tokenizers to count the number of tokens for the entire “imdb” dataset (drop the “unsupervised” part of the dataset). Enter the tokenizers in order such that the size of the dataset (measured in tokens) as returned by the tokenizers is in decreasing order. For example, if the first tokenizer yields the smallest number of tokens and the fourth tokenizer yields the largest, you would enter 1234 (without any spaces).”\n"]},{"cell_type":"code","execution_count":null,"id":"6eeb69fb","metadata":{"id":"6eeb69fb","outputId":"3de4ac06-28cb-482c-e786-efc9f60e4daf"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 50000\n","})"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# imds dataset\n","\n","imdb_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train+test\")\n","imdb_ds"]},{"cell_type":"code","execution_count":null,"id":"965887e3","metadata":{"id":"965887e3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"615ab0c3","metadata":{"id":"615ab0c3"},"outputs":[],"source":["def count_tokens(tokenizer, text):\n","    num_tokens = len(tokenizer.encode(text))\n","    return num_tokens"]},{"cell_type":"code","execution_count":null,"id":"f754b1d2","metadata":{"colab":{"referenced_widgets":["3fc387812daf48ec83bac2b282b6ae02"]},"id":"f754b1d2","outputId":"3dc3c6c5-10d1-4f5e-ff23-d5830c62ac78"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fc387812daf48ec83bac2b282b6ae02","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# 32K trained on 10 million\n","\n","token_count = imdb_ds.map(lambda x: {\"token_count\": count_tokens(tokenizer4, x[\"text\"])})"]},{"cell_type":"code","execution_count":null,"id":"240f1d4e","metadata":{"id":"240f1d4e","outputId":"c76b6502-152e-4acc-de4c-9e19d7a95aa2"},"outputs":[{"data":{"text/plain":["15352840"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["sum(token_count[\"token_count\"])"]},{"cell_type":"code","execution_count":null,"id":"8a272a4b","metadata":{"scrolled":false,"id":"8a272a4b"},"outputs":[],"source":["# bert-base-cased tokens\n","\n","tokens_bert = []\n","\n","token_count_bert = imdb_ds.map(lambda x:\n","                          {\"token_count\": count_tokens(bert_tokenizer, x[\"text\"])}\n","                        )"]},{"cell_type":"code","execution_count":null,"id":"d6a76cb0","metadata":{"id":"d6a76cb0","outputId":"9ca9a000-4ebc-455e-84d1-5f8969f6cff9"},"outputs":[{"data":{"text/plain":["15959815"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["sum(token_count_bert[\"token_count\"])"]},{"cell_type":"code","execution_count":null,"id":"4d891458","metadata":{"id":"4d891458"},"outputs":[],"source":["bert_tokenizer?"]},{"cell_type":"code","execution_count":null,"id":"771db69f","metadata":{"scrolled":true,"colab":{"referenced_widgets":["a220c68b9a9c4a189b1f16420d27b91c"]},"id":"771db69f","outputId":"be5206d3-d507-45c4-b951-fb8e5eb1983e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a220c68b9a9c4a189b1f16420d27b91c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["# gpt2 tokens\n","\n","\n","token_count_gpt2 = imdb_ds.map(lambda x:\n","                          {\"token_count\": count_tokens(gpt2_tokenizer, x[\"text\"])}\n","                         )"]},{"cell_type":"code","execution_count":null,"id":"f705a90c","metadata":{"id":"f705a90c","outputId":"1ec9c70e-e584-45ed-f772-4bf18c8c4082"},"outputs":[{"data":{"text/plain":["14812432"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["sum(token_count_gpt2[\"token_count\"])"]},{"cell_type":"code","execution_count":null,"id":"d51f3fd2","metadata":{"id":"d51f3fd2","outputId":"422e266c-34c7-4d40-8960-52f8806b4bdc"},"outputs":[{"data":{"text/plain":["{'gpt2': 14812432,\n"," 'hopper': 15347982,\n"," 'tokenizer4': 15352840,\n"," 'bert': 15959815}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["x = {\"tokenizer4\": 15352840,\n","\"bert\": 15959815,\n","\"gpt2\": 14812432,\n","\"hopper\": 15347982}\n","\n","sorted_x = dict(sorted(x.items(), key=lambda item: item[1]))\n","sorted_x"]},{"cell_type":"markdown","id":"9d27ed0c","metadata":{"id":"9d27ed0c"},"source":["3 gpt2\n","4 hopper\n","1 Custom tokenizer (vocab size 32K, trained on 10 million samples)\n","2 bert-base-uncased"]},{"cell_type":"code","execution_count":null,"id":"572bc1a7","metadata":{"id":"572bc1a7"},"outputs":[],"source":["# hopper\n","\n","token_count_hopper = imdb_ds.map(lambda x:\n","                          {\"token_count\": count_tokens(pt_tokenizer, x[\"text\"])}\n","                         )"]},{"cell_type":"code","execution_count":null,"id":"f14abdd9","metadata":{"id":"f14abdd9","outputId":"3130ccdc-0039-4919-f042-ada674df8c80"},"outputs":[{"data":{"text/plain":["15347982"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["sum(token_count_hopper[\"token_count\"])"]},{"cell_type":"markdown","id":"4e0030cf","metadata":{"id":"4e0030cf"},"source":["# Q7\n","\n","The statement that the special tokens and their respective token ids are model-specific (model here refers to a language model) is\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f5b0f3bb","metadata":{"id":"f5b0f3bb"},"outputs":[],"source":["# YES"]},{"cell_type":"markdown","id":"2aa8507d","metadata":{"id":"2aa8507d"},"source":["# Q8\n","\n","Suppose that the context length of the model is 128. Assume that a mini-batch of size 8 samples is passed to a tokenizer that corresponds to a model from hub. After tokenization, the maximum length of sample in the batch is 64. The statement that zero is appended to the “input ids” of the remaining samples to make the length 64 is\n","\n"]},{"cell_type":"code","execution_count":null,"id":"563ba3f0","metadata":{"id":"563ba3f0"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}