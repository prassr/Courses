{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Common Data\n",
        "\n",
        " Assume that we want to continue pertaining a model containing `7 billion parameters` with a context length of `2048` and the embedding dimension (model dimension) of `2048`. The dataset contains `1 million` samples (the size of each sample is exactly equal to the context length of the model). Suppose we set “per device batch size” to `16` and the gradient accumulation steps to `4`.\n"
      ],
      "metadata": {
        "id": "ifpLJX1mLUzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "\n",
        "How many iterations does it take to complete 1 epoch in a single GPU setting?"
      ],
      "metadata": {
        "id": "_BDx9ik-MMu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dbU4yxLHjMZ",
        "outputId": "21a4fa59-1356-40c5-9186-78401fecacc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250000.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "1_000_000 / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2\n",
        "\n",
        "The statement that the weight update happens at the end of one epoch is\n",
        "\n"
      ],
      "metadata": {
        "id": "p5-coNEEMgS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV42gf3zMhhp",
        "outputId": "1d9207c5-ed65-477b-a7fd-e80dc7102b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3\n",
        "\n",
        "How much memory (GB) is required to load the model with fp32?"
      ],
      "metadata": {
        "id": "Pqs2M8qnMiQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 bytes are required for each parameter\n",
        "# number of parameters multiplied by the data type used to store each parameter.\n",
        "\n",
        "mem_bytes = 7_000_000_000 * 4\n",
        "mem_in_gb = mem_bytes / 1e9\n",
        "mem_in_gb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAHchIrpMknZ",
        "outputId": "a1eef1e3-347f-45bb-d713-6d6efa748624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4\n",
        "\n",
        "Suppose the context length is increased from 2048 to 8000. Then, how much memory (GB) is required to load the model with fp32"
      ],
      "metadata": {
        "id": "Njiev4MzMmxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_size=2048*2048\n",
        "pos_new_size=8000*2048\n",
        "\n",
        "mem_in_gb = (7_000_000_000 - pos_size + pos_new_size) * 4 / 1e9\n",
        "mem_in_gb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb5GTAgwMn0v",
        "outputId": "d049ec65-2599-41d2-bd97-827b7804c4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.048758784"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5\n",
        "\n",
        "Suppose we fine-tune all the parameters of the model for Named Entity Recognition (NER) with a suitable dataset in a supervised manner.\n",
        "\n",
        "There are 8 labels (entities). Suppose we prefer to tune only the classification head (one linear layer with appropriate size). Enter the number of parameters in the classification head (exclude the bias).\n"
      ],
      "metadata": {
        "id": "LpEm0Uy9MpyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dimension * labels\n",
        "2048 * 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd691ZO7Mqr1",
        "outputId": "213d7aba-f901-44e8-e6e2-f6fe8ebb52cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16384"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7\n",
        "\n",
        "Task-specific (selective/partial) fine-tuning with a classification head takes far less memory than task-specific full-fine-tuning. The statement is\n"
      ],
      "metadata": {
        "id": "5KfDxYHtMsLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "True"
      ],
      "metadata": {
        "id": "Z0NBNru0MtWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8\n",
        "\n",
        "The training objective for both continued pre-training and Instruction tuning is the same. The statement is"
      ],
      "metadata": {
        "id": "_DhkqrkoMvrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "False"
      ],
      "metadata": {
        "id": "xs4wVNKaMxFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9\n",
        "\n",
        "Suppose we apply LoRA adapters to the model for parameter-efficient fine-tuning. Increasing the rank r will increase the number of parameters to be tuned. The statement is\n",
        "\n"
      ],
      "metadata": {
        "id": "BlL1XGKeMy1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "True"
      ],
      "metadata": {
        "id": "rt-5Drk9M1DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "[[1] How Epochs, Batch Sizes, Iterations, Steps, Total Steps and DataLoader Length Work?](https://medium.com/@sarubi_t/how-epochsbatch-sizes-iterations-steps-total-steps-and-dataloader-length-work-9150fa31fbca)\n",
        "\n",
        "[[2] Methods and tools for efficient training on a single GPU](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#batch-size-choice)"
      ],
      "metadata": {
        "id": "UbF6y0NPVtGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_mUzdlPVwuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}